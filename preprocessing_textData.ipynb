{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjV8u/OmaCGqQ/ascxKeqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nudrat-Habib/preprocessing-text-data/blob/main/preprocessing_textData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Natural Language Toolkit"
      ],
      "metadata": {
        "id": "FGFEp9uP9lDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
      ],
      "metadata": {
        "id": "39-bGPti9p7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #python library for NLP\n"
      ],
      "metadata": {
        "id": "pXtoLZQC94w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. About Twitter Dataset"
      ],
      "metadata": {
        "id": "vhTrTSF6-Y9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The rationale behind equal number of tweets is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.\n",
        "\n",
        " from nltk.corpus import twitter_samples.This will import three datasets from NLTK that contain various tweets to train and test the model:\n",
        "\n",
        "   1. negative_tweets.json: 5000 tweets with negative sentiments\n",
        "   2. positive_tweets.json: 5000 tweets with positive sentiments\n",
        "   3. tweets.20150430-223406.json: 20000 tweets with no sentiments"
      ],
      "metadata": {
        "id": "MzLqYtsr-cwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwUli6GC9VHh",
        "outputId": "e9077f0f-e3c8-455c-fedd-c674751f2b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "nltk.download (\"twitter_samples\") #sample dataset for classification."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets=twitter_samples.strings('negative_tweets.json')\n",
        "print('no of positive tweets',len(positive_tweets))\n",
        "print('no of negative tweets',len(negative_tweets))"
      ],
      "metadata": {
        "id": "zrefZByl-vRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be677c5-b661-45b0-d020-f0aa92d7d80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of positive tweets 5000\n",
            "no of negative tweets 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 printing some tweets"
      ],
      "metadata": {
        "id": "A6OYNyD4BiJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can print few tweets and explore dataset.we can use the index to print a spefic tweet or we can use random function to randomly select a tweet from dataset and the print it.\n",
        "we can also color code the tweets to make it more understandable."
      ],
      "metadata": {
        "id": "CfRYSjJDBm4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\033[92m\" + positive_tweets[15]) #will print positive tweet at location 15 in green color (code=92m)\n",
        "print(\"\\033[91m\" + negative_tweets[15]) #will print negative tweet at location 15 in red color (code=91m)\n",
        "import random\n",
        "# print positive in greeen\n",
        "print('\\033[92m' + positive_tweets[random.randint(0,5000)])\n",
        "\n",
        "# print negative in red\n",
        "print('\\033[91m' + negative_tweets[random.randint(0,5000)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMaNQrTaAbgt",
        "outputId": "ac7adf57-6f83-4588-c801-8c64c52821f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mLaying out a greetings card range for print today - love my job :-)\n",
            "\u001b[91mrelate to the \"sweet n' sour\" kind of \"bi-polar\" people in your life... cuz my life... is FULL of them... :(\n",
            "\u001b[92m@Kentsson have a wonderful day! Thank you :-)\n",
            "\u001b[91mFrom our investigations the answer is 'no'. Especially when the pups die within the free insurance period :( https://t.co/ZZRf0jKCeH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Preprocessing"
      ],
      "metadata": {
        "id": "BogYFdJwCn76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing, a component of data preparation, describes any type of processing performed on raw data to prepare it for another data processing procedure. for texual data some of the preprocessing techniques used and discussed in this notebook are;\n",
        "\n",
        "            1. Removing URL\n",
        "            2. Lower casing text\n",
        "            3. tokenization\n",
        "            4. POS tags\n",
        "            5. removing Stopwords\n",
        "            6. removing punctuations\n",
        "            6. stemming\n",
        "  \n",
        "  \n",
        "  in subsequesnt sections we will discuss these briefly and then will apply it to twitter dataset.\n",
        "        \n",
        "  "
      ],
      "metadata": {
        "id": "GH2dEMFpAzCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Removing URL\n",
        " we first need to import re module which stands for \"regular expressions.\" Regular expressions are a powerful tool for pattern matching and manipulation of strings.\n",
        " it provides various methods like;\n",
        "1. re.search(pattern, string): Searches for the first occurrence of the string.\n",
        "2. re.match(pattern, string): Checks if the pattern matches at the beginning of the string.\n",
        "3. re.sub(pattern, replacement, string): Replaces occurrences of the pattern in the string with the replacement. and so on.\n",
        "for URL removal we remove url and replace it with ' '."
      ],
      "metadata": {
        "id": "CJJOhRCKB4T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "corpus=\"my name is nudrat and i am interested in '#NLP'. My linkedin profile is https://www.linkedin.com/in/nudrat-habib-a3486a234/.it's a beautiful day coming next \"\n",
        "print('corpus with URL:',corpus)\n",
        "new_corpus=re.sub(r'http\\S+','',corpus)\n",
        "print('corpus with URL removed',new_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PgHG-SNCFtt",
        "outputId": "334aa323-60a7-424a-c976-9e5a16986837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus with URL: my name is nudrat and i am interested in '#NLP'. My linkedin profile is https://www.linkedin.com/in/nudrat-habib-a3486a234/.it's a beautiful day coming next \n",
            "corpus with URL removed my name is nudrat and i am interested in '#NLP'. My linkedin profile is  a beautiful day coming next \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Lower casing\n",
        "Nextt we convert all the text to lower case."
      ],
      "metadata": {
        "id": "kmraNFKOD6fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=new_corpus.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTb6Vrv3EBS1",
        "outputId": "99bc1867-ce3b-4b06-c4ac-b17c3392cda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name is nudrat and i am interested in '#nlp'. my linkedin profile is  a beautiful day coming next \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Tokenization\n",
        "Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n",
        "we will convert the text into words/tokens."
      ],
      "metadata": {
        "id": "ftRljzA3Ht9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(corpus) #2. converting sentence into tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EG0-_a0ICoL",
        "outputId": "6c347aa2-74cb-46de-f3fc-bc8a876741c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'name', 'is', 'nudrat', 'and', 'i', 'am', 'interested', 'in', \"'\", '#', 'nlp', \"'\", '.', 'my', 'linkedin', 'profile', 'is', 'a', 'beautiful', 'day', 'coming', 'next']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Removing Punctuations and stopwords.\n",
        "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead.\n",
        "punctions also are insignifacnt to data. so we remove both stopwords and punctuations from data."
      ],
      "metadata": {
        "id": "j7SO2GSXIUUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punct')\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "clean_data=[]\n",
        "stopwords=stopwords.words('english')\n",
        "for word in tokens:\n",
        "  if (word not in stopwords and word not in string.punctuation): # 4 stopwords 5. punctuations\n",
        "    clean_data.append(word)\n",
        "print(clean_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnYWOaW3z0wM",
        "outputId": "296e636a-0d6b-41eb-9591-169b0d8ee435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['name', 'nudrat', 'interested', 'nlp', 'linkedin', 'profile', 'beautiful', 'day', 'coming', 'next']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading punct: Package 'punct' not found in index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.5 Stemmer\n",
        "Stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The process of stemming is used to normalize text and make it easier to process. It is an important step in text pre-processing, and it is commonly used in information retrieval and text mining applications.\n",
        "The input to the stemmer is tokenized words."
      ],
      "metadata": {
        "id": "gYN_pnJILgVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "print('clean words:',clean_data)\n",
        "stem_words=[]\n",
        "for word in clean_data:\n",
        "  stem_word=stemmer.stem(word)\n",
        "  stem_words.append(stem_word)\n",
        "print('stemmed words: ',stem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIVE7zLSiXWn",
        "outputId": "0544845a-c4aa-41f7-a873-82491e1a2e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clean words: ['name', 'nudrat', 'interested', 'nlp', 'linkedin', 'profile', 'beautiful', 'day', 'coming', 'next']\n",
            "stemmed words:  ['name', 'nudrat', 'interest', 'nlp', 'linkedin', 'profil', 'beauti', 'day', 'come', 'next']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that we are familiar with the preprocessing, we will apply these techniques to twitter dataset.\n",
        "first we familiraze ourself with the code, now we will create functions for each preprocessing step and then preprocessed twitter data.\n",
        "all the preprocessing steps are included inside function."
      ],
      "metadata": {
        "id": "54pR-yPDNFJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string #for string operations\n",
        "import re #for  maipulation of string e.g URL removal\n",
        "from nltk.stem import PorterStemmer #for stemming\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punct')\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "def clean_data(data):\n",
        "  stemmer = PorterStemmer()\n",
        "  stopword = stopwords.words('english')\n",
        "  data=re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', data)                   #removing hyperlinks\n",
        "  data=re.sub(r'@#', '', data)\n",
        "  corpus=data.lower()                                        #lower casing\n",
        "  tokens = nltk.word_tokenize(data)                               #converting sentence into token\n",
        "  clean_data=[]\n",
        "  for word in tokens:\n",
        "      if (word not in stopword and word not in string.punctuation): #removing stopwords and punctuations\n",
        "        clean_data.append(word)\n",
        "\n",
        "  stem_words=[]\n",
        "  for word in clean_data:                                           #stemming\n",
        "      stem_word=stemmer.stem(word)\n",
        "      stem_words.append(stem_word)\n",
        "  return stem_words\n"
      ],
      "metadata": {
        "id": "G94y30MROiXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now let's just check if the function works properly on a sample tweet before applying it to entire dataset."
      ],
      "metadata": {
        "id": "o5d6u2oNqIfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=\"you are so beautiful enjoy your life, keep smiling and join me here  https://www.facebook.com/ \"\n",
        "clean_tweet=clean_data(tweet)\n",
        "print(clean_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8KBafcNqUAO",
        "outputId": "dfee7679-1b9d-48ff-9acb-254aa0cf8c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['beauti', 'enjoy', 'life', 'keep', 'smile', 'join']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finally we will iterate on positive tweets and negative tweets and preprocess each tweet."
      ],
      "metadata": {
        "id": "OPcv5C0QrV1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_pos_tweets = []\n",
        "cleaned_neg_tweets=[]\n",
        "for tweet in positive_tweets:\n",
        "    cleaned_tweet = clean_data(tweet)\n",
        "    cleaned_pos_tweets.append(cleaned_tweet)\n",
        "for tweet in negative_tweets:\n",
        "  cleaned_tweet=clean_data(tweet)\n",
        "  cleaned_neg_tweets.append(cleaned_tweet)\n"
      ],
      "metadata": {
        "id": "1k2PXCrSoCL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's just print one tweet before and after cleaning from postive tweets and negative tweets."
      ],
      "metadata": {
        "id": "TjJ8MWD2tczZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('positive tweet: ',positive_tweets[27])\n",
        "print('cleaned positive tweet',cleaned_pos_tweets[127])\n",
        "\n",
        "\n",
        "print('\\n\\n negative tweet:',negative_tweets[15])\n",
        "print('cleaned negative tweet',cleaned_neg_tweets[15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36pVIROooOwz",
        "outputId": "9637bc66-fece-4560-b10c-9c76aed80440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive tweet:  Spiritual Ritual Festival (Népal)\n",
            "Beginning of Line-up :)\n",
            "It is left for the line-up (y)\n",
            "See more at:... http://t.co/QMNz62OEuc\n",
            "cleaned positive tweet ['followfriday', 'michelploria', 'myfrenchc', 'jasoncr', 'top', 'new', 'follow', 'commun', 'week']\n",
            "\n",
            "\n",
            " negative tweet: relate to the \"sweet n' sour\" kind of \"bi-polar\" people in your life... cuz my life... is FULL of them... :(\n",
            "cleaned negative tweet ['relat', '``', 'sweet', 'n', 'sour', \"''\", 'kind', '``', 'bi-polar', \"''\", 'peopl', 'life', '...', 'cuz', 'life', '...', 'full', '...']\n"
          ]
        }
      ]
    }
  ]
}